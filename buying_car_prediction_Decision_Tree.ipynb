{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set from : https://www.kaggle.com/code/casper6290/car-purchase-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cdd886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-TQA2F8M:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>decision tree</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d9b4b18100>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.appName(\"decision tree\").getOrCreate()\n",
    "spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5fbd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "+-------+------+---+------------+---------+\n",
      "|User ID|Gender|Age|AnnualSalary|Purchased|\n",
      "+-------+------+---+------------+---------+\n",
      "|    385|  Male| 35|       20000|        0|\n",
      "|    681|  Male| 40|       43500|        0|\n",
      "|    353|  Male| 49|       74000|        0|\n",
      "|    895|  Male| 40|      107500|        1|\n",
      "|    661|  Male| 25|       79000|        0|\n",
      "|    846|Female| 47|       33500|        1|\n",
      "|    219|Female| 46|      132500|        1|\n",
      "|    588|  Male| 42|       64000|        0|\n",
      "|     85|Female| 30|       84500|        0|\n",
      "|    465|  Male| 41|       52000|        0|\n",
      "|    686|  Male| 42|       80000|        0|\n",
      "|    408|  Male| 47|       23000|        1|\n",
      "|    790|Female| 32|       72500|        0|\n",
      "|    116|Female| 27|       57000|        0|\n",
      "|    118|Female| 42|      108000|        1|\n",
      "|     54|Female| 33|      149000|        1|\n",
      "|     90|  Male| 35|       75000|        0|\n",
      "|    372|  Male| 35|       53000|        0|\n",
      "|    926|  Male| 46|       79000|        1|\n",
      "|     94|Female| 39|      134000|        1|\n",
      "+-------+------+---+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading the data\n",
    "data = spark_session.read.csv(\"car_data.csv\", inferSchema = True, header = True)\n",
    "print(data.count())\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8022e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+------------+---------+--------------+\n",
      "|User ID|Gender|Age|AnnualSalary|Purchased|Gender_indexed|\n",
      "+-------+------+---+------------+---------+--------------+\n",
      "|    385|  Male| 35|       20000|        0|           1.0|\n",
      "|    681|  Male| 40|       43500|        0|           1.0|\n",
      "|    353|  Male| 49|       74000|        0|           1.0|\n",
      "|    895|  Male| 40|      107500|        1|           1.0|\n",
      "|    661|  Male| 25|       79000|        0|           1.0|\n",
      "|    846|Female| 47|       33500|        1|           0.0|\n",
      "|    219|Female| 46|      132500|        1|           0.0|\n",
      "|    588|  Male| 42|       64000|        0|           1.0|\n",
      "|     85|Female| 30|       84500|        0|           0.0|\n",
      "|    465|  Male| 41|       52000|        0|           1.0|\n",
      "|    686|  Male| 42|       80000|        0|           1.0|\n",
      "|    408|  Male| 47|       23000|        1|           1.0|\n",
      "|    790|Female| 32|       72500|        0|           0.0|\n",
      "|    116|Female| 27|       57000|        0|           0.0|\n",
      "|    118|Female| 42|      108000|        1|           0.0|\n",
      "|     54|Female| 33|      149000|        1|           0.0|\n",
      "|     90|  Male| 35|       75000|        0|           1.0|\n",
      "|    372|  Male| 35|       53000|        0|           1.0|\n",
      "|    926|  Male| 46|       79000|        1|           1.0|\n",
      "|     94|Female| 39|      134000|        1|           0.0|\n",
      "+-------+------+---+------------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transforming the gender to int using string indexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "string_indexer = StringIndexer(inputCol=\"Gender\" , outputCol=\"Gender_indexed\")\n",
    "indexed_data = string_indexer.fit(data).transform(data)\n",
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1029e616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|Purchased|\n",
      "+--------------------+---------+\n",
      "|[385.0,35.0,20000...|        0|\n",
      "|[681.0,40.0,43500...|        0|\n",
      "|[353.0,49.0,74000...|        0|\n",
      "|[895.0,40.0,10750...|        1|\n",
      "|[661.0,25.0,79000...|        0|\n",
      "|[846.0,47.0,33500...|        1|\n",
      "|[219.0,46.0,13250...|        1|\n",
      "|[588.0,42.0,64000...|        0|\n",
      "|[85.0,30.0,84500....|        0|\n",
      "|[465.0,41.0,52000...|        0|\n",
      "|[686.0,42.0,80000...|        0|\n",
      "|[408.0,47.0,23000...|        1|\n",
      "|[790.0,32.0,72500...|        0|\n",
      "|[116.0,27.0,57000...|        0|\n",
      "|[118.0,42.0,10800...|        1|\n",
      "|[54.0,33.0,149000...|        1|\n",
      "|[90.0,35.0,75000....|        0|\n",
      "|[372.0,35.0,53000...|        0|\n",
      "|[926.0,46.0,79000...|        1|\n",
      "|[94.0,39.0,134000...|        1|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assembling the features using vectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(inputCols = [\"User ID\",\"Age\",\"AnnualSalary\",\"Gender_indexed\"] , outputCol = \"features\")\n",
    "finalized_data = va.transform(indexed_data)\n",
    "finalized_data = finalized_data.select([\"features\" , \"Purchased\"])\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9610b19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n",
      "714\n"
     ]
    }
   ],
   "source": [
    "#spliting the data  : test , train\n",
    "test_data , train_data = finalized_data.randomSplit([0.3,0.7])\n",
    "print(test_data.count())\n",
    "print(train_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "105e2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model and training it\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(labelCol = \"Purchased\")\n",
    "classifier = classifier.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e8115bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------+--------------------+----------+\n",
      "|            features|Purchased|rawPrediction|         probability|prediction|\n",
      "+--------------------+---------+-------------+--------------------+----------+\n",
      "|[1.0,32.0,100000....|        1|   [11.0,8.0]|[0.57894736842105...|       0.0|\n",
      "|[7.0,51.0,134000....|        0|  [10.0,66.0]|[0.13157894736842...|       1.0|\n",
      "|[8.0,54.0,26000.0...|        1|   [3.0,42.0]|[0.06666666666666...|       1.0|\n",
      "|[9.0,46.0,33500.0...|        1|   [3.0,42.0]|[0.06666666666666...|       1.0|\n",
      "|[10.0,24.0,64500....|        0|  [232.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|[13.0,29.0,80000....|        0|   [79.0,1.0]|     [0.9875,0.0125]|       0.0|\n",
      "|[14.0,47.0,60500....|        0|  [24.0,19.0]|[0.55813953488372...|       0.0|\n",
      "|[15.0,48.0,26500....|        1|   [3.0,42.0]|[0.06666666666666...|       1.0|\n",
      "|[16.0,55.0,152500...|        1|  [10.0,66.0]|[0.13157894736842...|       1.0|\n",
      "|[18.0,63.0,44500....|        1|    [0.0,3.0]|           [0.0,1.0]|       1.0|\n",
      "|[19.0,48.0,31500....|        1|   [3.0,42.0]|[0.06666666666666...|       1.0|\n",
      "|[21.0,36.0,62500....|        0|  [232.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|[24.0,37.0,93000....|        1|   [3.0,22.0]|         [0.12,0.88]|       1.0|\n",
      "|[25.0,49.0,34500....|        1|   [3.0,42.0]|[0.06666666666666...|       1.0|\n",
      "|[32.0,47.0,50000....|        1|  [24.0,19.0]|[0.55813953488372...|       0.0|\n",
      "|[36.0,20.0,26500....|        0|  [232.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|[38.0,43.0,76500....|        0|  [57.0,16.0]|[0.78082191780821...|       0.0|\n",
      "|[39.0,19.0,83500....|        0|   [79.0,1.0]|     [0.9875,0.0125]|       0.0|\n",
      "|[40.0,26.0,86000....|        0|   [79.0,1.0]|     [0.9875,0.0125]|       0.0|\n",
      "|[41.0,36.0,78500....|        0|   [79.0,1.0]|     [0.9875,0.0125]|       0.0|\n",
      "+--------------------+---------+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the predictions\n",
    "predictions = classifier.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41852e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9347490347490347"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eavluating the accuracy \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bca = BinaryClassificationEvaluator(labelCol = \"Purchased\")\n",
    "bca.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb0672",
   "metadata": {},
   "source": [
    "# the accuarcy using decision tree algorithm in this case is better than the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4b8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
